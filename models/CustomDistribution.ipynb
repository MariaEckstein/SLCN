# Data: 2 trials, 2 actions
seasons = np.array([0, 0])
aliens = np.array([0, 0])
actions = np.array([0, 1])
rewards = np.array([1, 1])

n_trials = 2
n_aliens = 2
n_actions = 2
n_seasons = 2
n_TS = 2

aliens = theano.shared(np.asarray(aliens, dtype='int32'))
actions = theano.shared(np.asarray(actions, dtype='int32'))
rewards = theano.shared(np.asarray(rewards, dtype='int32'))

class CustomTSDist(pm.DiscreteUniform):
    def __init__(self, lower, upper, z, *args, **kwargs):
        super(CustomTSDist, self).__init__(lower, upper, *args, **kwargs)
        self.z = z  # Raw values as inputs for softmax (len = len(TS domain))
        self.lower = lower
        self.upper = upper
        
    def logp(self, value):
        upper = self.upper
        lower = self.lower
        # bound bounds a distribution, takes probability computation as first argument, bounds as 2nd/3rd
        p = np.exp(self.z)/np.sum(np.exp(self.z))  # Softmax
        return pm.distributions.dist_math.bound(-T.log(p), lower <= value, value <= upper)

    
with pm.Model() as model:
    
    # RL parameters
    alpha = pm.Uniform('alpha', lower=0, upper=1)
    beta = pm.Bound(pm.Normal, lower=0)('beta', mu=1, sd=5)
    T.printing.Print('alpha')(alpha)
    T.printing.Print('beta')(beta)
    
    # Initial Q-values
    Q_high0 = 0.5 * T.ones([n_seasons, n_TS])  # Q-values linking seasons to TS
    Q_low0 = 0.5 * T.ones([n_TS, n_aliens, n_actions])  # Q-values linking TS & aliens to actions
    T.printing.Print('Q_high0')(Q_high0)
    T.printing.Print('Q_low0')(Q_low0)
    
    # Set up TS distribution (softmax)
    TS = CustomTSDist('TS', 0, 2, z=[3, 4, 9])

    # Trial 0
    ## Select TS
    p_high0 = T.nnet.softmax(Q_high0[seasons[0]])
    T.printing.Print('p_high0')(p_high0.flatten())

    
    ## Calculate action values for this trial
    p_low0 = T.nnet.softmax(Q_low0[TS])
    T.printing.Print('p_low0')(p_low0)

    ## Calculate RPEs
    now_high = seasons[0], TS
    now_low = TS, aliens[0], actions[0]

    RPE_high = rewards[0] - Q_high0[now_high]  # calculate RPE of the first trial
    RPE_low = rewards[0] - Q_low0[now_low]  # calculate RPE of the first trial
    
    T.printing.Print('RPE_high')(RPE_high)
    T.printing.Print('RPE_low')(RPE_low)
    
    ## Update Q-values
    Q_high1 = T.set_subtensor(Q_high0[now_high],
                              Q_high0[now_high] + alpha * RPE_high)
    Q_low1 = T.set_subtensor(Q_low0[now_low],
                             Q_low0[now_low] + alpha * RPE_low)
    
    T.printing.Print('Q_high1')(Q_high1)
    T.printing.Print('Q_low1')(Q_low1)
    
    # Trial 1
    ## Select TS
    p_high1 = T.nnet.softmax(Q_high1[seasons[1]])
    T.printing.Print('p_high1')(p_high1.flatten())

    ## Calculate action values for this trial
    p_low1 = T.nnet.softmax(Q_low1[TS])
    T.printing.Print('p_low1')(p_low1)

    ## Calculate RPEs
    now_high = seasons[1], TS
    now_low = TS, aliens[1], actions[1]
    
    RPE_high = rewards[1] - Q_high1[now_high]  # calculate RPE of the first trial
    RPE_low = rewards[1] - Q_low1[now_low]  # calculate RPE of the first trial
    
    T.printing.Print('RPE_high')(RPE_high)
    T.printing.Print('RPE_low')(RPE_low)
    
    ## Update Q-values
    Q_high2 = T.set_subtensor(Q_high1[now_high],
                              Q_high1[now_high] + alpha * RPE_high)
    Q_low2 = T.set_subtensor(Q_low1[now_low],
                             Q_low1[now_low] + alpha * RPE_low)
    T.printing.Print('Q_high2')(Q_high2)
    T.printing.Print('Q_low2')(Q_low2)
        
    # Select actions
    Q_low_all = T.concatenate([Q_low0[TS, aliens[0]],
                               Q_low1[TS, aliens[1]]]).reshape((n_trials, n_actions))
    T.printing.Print('Q_low_all')(Q_low_all)
    
    p_low_all = T.concatenate([p_low0[aliens[0]],
                               p_low1[aliens[1]]]).reshape((n_trials, n_actions))
    T.printing.Print('p_low_all')(p_low_all)
    
    actions = pm.Categorical('actions', p_low_all, observed=actions)